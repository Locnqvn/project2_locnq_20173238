{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"mount_file_id":"1V-MFN9L_N-Y7nIWpShgFv5WXBb4A_Il9","authorship_tag":"ABX9TyP20ZXD/fDdpLJ6tK9WVheb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"O4uxsQvxYQet","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random\n","\n","class DataReader:\n","    def __init__(self,path,batch_size):\n","        with open(path,'r') as f:\n","            d_line=f.read().splitlines()\n","        self.x=[]\n","        self.y=[]\n","        self.batch_size=batch_size\n","        for d in d_line:\n","            fea=d.split('<fff>')\n","            label,sentence_len,data=int(fea[0]),int(fea[2]),fea[-1]\n","            self.y.append(label)\n","            vector=[int(number) for number in data.split()]\n","            self.x.append(np.array(vector))\n","        \n","        self.n_doc=len(d_line)\n","        self.n_batch=int(np.ceil(self.n_doc/batch_size))\n","        rd=random.sample(range(self.n_doc),self.n_doc)\n","        self.x=np.array(self.x)[rd]\n","        self.y=np.array(self.y)[rd]\n","        \n","        self.batch_id=0\n","        self.epoch=0\n","    \n","    def next_batch(self):\n","        self.batch_id+=1\n","        if(self.batch_id>=self.n_batch):\n","            self.batch_id=0\n","            self.epoch+=1\n","            rd=random.sample(range(self.n_doc),self.n_doc)\n","            self.x=self.x[rd]\n","            self.y=self.y[rd]\n","            \n","        start=self.batch_id*self.batch_size\n","        end=start+self.batch_size\n","        return self.x[start:end],self.y[start:end]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8QwYukrYmeI","colab_type":"code","colab":{}},"source":["path_train='/content/drive/My Drive/Datasets/trainencode.txt'\n","path_test='/content/drive/My Drive/Datasets/testencode.txt'\n","path_vocab='/content/drive/My Drive/Datasets/vocab.txt'\n","\n","with open(path_vocab,'r') as f:\n","    vocab_size=len(f.read().splitlines())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DoGAnNe1JA39","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ConvNet(nn.Module):\n","    def __init__(self,num_class,vocab_size,embed_size,kernel_height):\n","        super(ConvNet,self).__init__()\n","        num_feamap=500  # channel out\n","        self.embed=nn.Embedding(vocab_size,embed_size)\n","        self.conv=nn.Conv2d(\n","            in_channels=1,\n","            out_channels=num_feamap,\n","            kernel_size=(kernel_height,embed_size),\n","            stride=1\n","        )\n","        self.dropout=nn.Dropout(0.5)\n","        self.fc=nn.Linear(num_feamap,num_class)\n","\n","    def forward(self,x):\n","        # embedding data\n","        x=self.embed(x)  #[batch_size,sentence_len,embed_size]\n","        x=x.unsqueeze(1) #[batch_size,1,sentence_len,embed_size]\n","        x= F.relu(self.conv(x)).squeeze(3) # [batch_size,channel_size,heights_out]\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2) #[batch_size,channels_size]\n","        x=self.dropout(x)\n","        logits=self.fc(x)\n","        return logits\n","\n","\n","class CNN_text_classification:\n","    def __init__(self,vocab_size,embed_size,batch_size,num_class\n","        ,kernel_heights,lr=0.001):\n","        self._vocab_size=vocab_size\n","        self._batch_size=batch_size\n","        self._num_class=num_class\n","        self._embed_size=embed_size\n","        self._kernel_heights=kernel_heights\n","        self._lr=lr\n","        self._device = torch.device('cuda:0')\n","        self._data_reader=None\n","        self._labels=None\n","        self.net=None\n","    \n","    def fit(self,path_train,max_iter):\n","        # read data\n","        self._data_reader=DataReader(path_train,50)\n","        self._net=ConvNet(self._num_class,self._vocab_size,self._embed_size,self._kernel_heights)\n","        # network devices set with gpu\n","        self._net.to(self._device)\n","        #loss function\n","        self._criterion=nn.CrossEntropyLoss()\n","        # optimizer Adam\n","        self._optimizer=torch.optim.Adam(self._net.parameters(),self._lr)\n","        # train state\n","        self._net.train()\n","        it=0\n","        while it<max_iter:\n","            # read data\n","            train_data,train_labels=self._data_reader.next_batch()\n","            # convert data to tensor\n","            train_data=torch.from_numpy(train_data).to(self._device)\n","            train_labels=torch.from_numpy(train_labels).to(self._device)\n","\n","            # clear gradient\n","            self._optimizer.zero_grad()\n","\n","            # train\n","            labels_pre=self._net(train_data)\n","\n","            # calculate loss\n","            loss=self._criterion(labels_pre,train_labels)\n","\n","            # calculate gradient\n","            loss.backward()\n","\n","            # update parameters\n","            self._optimizer.step()\n","\n","            if(self._data_reader.batch_id%50==0):\n","                print(\"epoch{}, step {} loss = {}\".format(\n","                    self._data_reader.epoch,self._data_reader.batch_id,loss.item()))\n","            it+=1\n","\n","    def score(self,path_test):\n","        self._net.eval() # train state\n","        self.test_datareader=DataReader(path_test,50) # read data\n","        num_pre_true=0\n","        while True:\n","            batch_test_data,batch_test_labels=self.test_datareader.next_batch()\n","            # convert data to tensor\n","            batch_test_data=torch.from_numpy(batch_test_data).to(self._device)\n","            batch_test_labels=torch.from_numpy(batch_test_labels).to(self._device)\n","            pre_vals = self._net(batch_test_data)\n","            num_pre_true+=(torch.max(pre_vals.data, 1)[1].view(batch_test_labels.size()).data==batch_test_labels.data).sum()\n","            if(self.test_datareader.epoch==1):\n","                break\n","    \n","        return (num_pre_true*100.0/len(self.test_datareader.y)).cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLyS5VgUYvUt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592573538910,"user_tz":-420,"elapsed":204479,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"outputId":"068abdb0-9988-4867-bc6e-49c7054c56f2"},"source":["import time\n","t=time.time()\n","model=CNN_text_classification(vocab_size+2,100,50,20,3,lr=0.001)\n","model.fit(path_train,6000)\n","print('time train= ',time.time()-t)\n","print('accuracy train= ',model.score(path_train))\n","print('accuracy test= ',model.score(path_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch0, step 50 loss = 3.1127166748046875\n","epoch0, step 100 loss = 2.8797240257263184\n","epoch0, step 150 loss = 2.7763733863830566\n","epoch0, step 200 loss = 2.7854814529418945\n","epoch1, step 0 loss = 2.6540956497192383\n","epoch1, step 50 loss = 2.1089468002319336\n","epoch1, step 100 loss = 2.09087872505188\n","epoch1, step 150 loss = 2.417496681213379\n","epoch1, step 200 loss = 1.8510149717330933\n","epoch2, step 0 loss = 1.6250605583190918\n","epoch2, step 50 loss = 1.8634411096572876\n","epoch2, step 100 loss = 1.3512171506881714\n","epoch2, step 150 loss = 1.9497559070587158\n","epoch2, step 200 loss = 1.3958373069763184\n","epoch3, step 0 loss = 1.0822604894638062\n","epoch3, step 50 loss = 0.9988605976104736\n","epoch3, step 100 loss = 1.4059709310531616\n","epoch3, step 150 loss = 1.381475806236267\n","epoch3, step 200 loss = 1.1934213638305664\n","epoch4, step 0 loss = 1.0425304174423218\n","epoch4, step 50 loss = 0.9299280643463135\n","epoch4, step 100 loss = 0.9522096514701843\n","epoch4, step 150 loss = 1.0394963026046753\n","epoch4, step 200 loss = 1.0395463705062866\n","epoch5, step 0 loss = 1.1266194581985474\n","epoch5, step 50 loss = 0.7145079970359802\n","epoch5, step 100 loss = 1.0318855047225952\n","epoch5, step 150 loss = 0.6378690004348755\n","epoch5, step 200 loss = 0.7953060865402222\n","epoch6, step 0 loss = 0.7007911205291748\n","epoch6, step 50 loss = 1.1662181615829468\n","epoch6, step 100 loss = 0.7994236946105957\n","epoch6, step 150 loss = 0.7576771378517151\n","epoch6, step 200 loss = 0.7183414697647095\n","epoch7, step 0 loss = 0.4842202663421631\n","epoch7, step 50 loss = 0.6113620400428772\n","epoch7, step 100 loss = 0.48316246271133423\n","epoch7, step 150 loss = 0.49951499700546265\n","epoch7, step 200 loss = 0.5215363502502441\n","epoch8, step 0 loss = 0.5890015959739685\n","epoch8, step 50 loss = 0.4699062407016754\n","epoch8, step 100 loss = 0.49618664383888245\n","epoch8, step 150 loss = 0.6107075810432434\n","epoch8, step 200 loss = 0.25982552766799927\n","epoch9, step 0 loss = 0.28488513827323914\n","epoch9, step 50 loss = 0.32173386216163635\n","epoch9, step 100 loss = 0.18393239378929138\n","epoch9, step 150 loss = 0.4922289252281189\n","epoch9, step 200 loss = 0.6018671989440918\n","epoch10, step 0 loss = 0.3492695987224579\n","epoch10, step 50 loss = 0.30105000734329224\n","epoch10, step 100 loss = 0.3495829403400421\n","epoch10, step 150 loss = 0.2868456542491913\n","epoch10, step 200 loss = 0.4518581032752991\n","epoch11, step 0 loss = 0.26274797320365906\n","epoch11, step 50 loss = 0.4003313183784485\n","epoch11, step 100 loss = 0.2298765629529953\n","epoch11, step 150 loss = 0.41878947615623474\n","epoch11, step 200 loss = 0.29314154386520386\n","epoch12, step 0 loss = 0.14641378819942474\n","epoch12, step 50 loss = 0.1954491287469864\n","epoch12, step 100 loss = 0.22142814099788666\n","epoch12, step 150 loss = 0.1665312796831131\n","epoch12, step 200 loss = 0.235467791557312\n","epoch13, step 0 loss = 0.3081141412258148\n","epoch13, step 50 loss = 0.12218665331602097\n","epoch13, step 100 loss = 0.26475560665130615\n","epoch13, step 150 loss = 0.3196420669555664\n","epoch13, step 200 loss = 0.13344956934452057\n","epoch14, step 0 loss = 0.3121979236602783\n","epoch14, step 50 loss = 0.08717188984155655\n","epoch14, step 100 loss = 0.05499600991606712\n","epoch14, step 150 loss = 0.20133323967456818\n","epoch14, step 200 loss = 0.19877713918685913\n","epoch15, step 0 loss = 0.182098388671875\n","epoch15, step 50 loss = 0.06758588552474976\n","epoch15, step 100 loss = 0.08325628191232681\n","epoch15, step 150 loss = 0.05921189859509468\n","epoch15, step 200 loss = 0.09749159216880798\n","epoch16, step 0 loss = 0.0573398694396019\n","epoch16, step 50 loss = 0.06410609185695648\n","epoch16, step 100 loss = 0.03712160140275955\n","epoch16, step 150 loss = 0.03995565325021744\n","epoch16, step 200 loss = 0.24202358722686768\n","epoch17, step 0 loss = 0.03673180937767029\n","epoch17, step 50 loss = 0.13216020166873932\n","epoch17, step 100 loss = 0.1367368847131729\n","epoch17, step 150 loss = 0.07873232662677765\n","epoch17, step 200 loss = 0.1358834207057953\n","epoch18, step 0 loss = 0.07416114956140518\n","epoch18, step 50 loss = 0.08606567233800888\n","epoch18, step 100 loss = 0.08377742022275925\n","epoch18, step 150 loss = 0.1474624127149582\n","epoch18, step 200 loss = 0.02570987306535244\n","epoch19, step 0 loss = 0.026483550667762756\n","epoch19, step 50 loss = 0.08611000329256058\n","epoch19, step 100 loss = 0.09522636234760284\n","epoch19, step 150 loss = 0.03985811769962311\n","epoch19, step 200 loss = 0.031326815485954285\n","epoch20, step 0 loss = 0.11539378017187119\n","epoch20, step 50 loss = 0.10670775175094604\n","epoch20, step 100 loss = 0.06844420731067657\n","epoch20, step 150 loss = 0.05578627437353134\n","epoch20, step 200 loss = 0.09983763098716736\n","epoch21, step 0 loss = 0.011521244421601295\n","epoch21, step 50 loss = 0.031142445281147957\n","epoch21, step 100 loss = 0.012032585218548775\n","epoch21, step 150 loss = 0.07163891941308975\n","epoch21, step 200 loss = 0.03179517388343811\n","epoch22, step 0 loss = 0.05464136227965355\n","epoch22, step 50 loss = 0.008969392627477646\n","epoch22, step 100 loss = 0.085051991045475\n","epoch22, step 150 loss = 0.044420674443244934\n","epoch22, step 200 loss = 0.010532579384744167\n","epoch23, step 0 loss = 0.04453421011567116\n","epoch23, step 50 loss = 0.018343420699238777\n","epoch23, step 100 loss = 0.06041764095425606\n","epoch23, step 150 loss = 0.05613565444946289\n","epoch23, step 200 loss = 0.03905933350324631\n","epoch24, step 0 loss = 0.10288838297128677\n","epoch24, step 50 loss = 0.02369118668138981\n","epoch24, step 100 loss = 0.028462562710046768\n","epoch24, step 150 loss = 0.06645841151475906\n","epoch24, step 200 loss = 0.0424833782017231\n","epoch25, step 0 loss = 0.024602951481938362\n","epoch25, step 50 loss = 0.00823194533586502\n","epoch25, step 100 loss = 0.008821172639727592\n","epoch25, step 150 loss = 0.009877176024019718\n","epoch25, step 200 loss = 0.07921188324689865\n","epoch26, step 0 loss = 0.06948788464069366\n","epoch26, step 50 loss = 0.004794406704604626\n","time train=  194.89620232582092\n","accuracy train=  99.95581\n","accuracy test=  78.79713\n"],"name":"stdout"}]}]}
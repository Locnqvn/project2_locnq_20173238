{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1anTCu0eD9qRzAltPEk1YdilouiRiGgMF","authorship_tag":"ABX9TyPtPmI/JdEa9YbReiMFjiKH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"66prbf1R1I3A","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592551327521,"user_tz":-420,"elapsed":750,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["import numpy as np\n","import random\n","\n","class DataReader:\n","    def __init__(self,path,batch_size):\n","        with open(path,'r') as f:\n","            d_line=f.read().splitlines()\n","        self.x=[]\n","        self.y=[]\n","        self.batch_size=batch_size\n","        for d in d_line:\n","            fea=d.split('<fff>')\n","            label,sentence_len,data=int(fea[0]),int(fea[2]),fea[-1]\n","            self.y.append(label)\n","            vector=[int(number) for number in data.split()]\n","            self.x.append(np.array(vector))\n","        \n","        self.n_doc=len(d_line)\n","        self.n_batch=int(np.ceil(self.n_doc/batch_size))\n","        rd=random.sample(range(self.n_doc),self.n_doc)\n","        self.x=np.array(self.x)[rd]\n","        self.y=np.array(self.y)[rd]\n","        \n","        self.batch_id=0\n","        self.epoch=0\n","    \n","    def next_batch(self):\n","        self.batch_id+=1\n","        if(self.batch_id>=self.n_batch):\n","            self.batch_id=0\n","            self.epoch+=1\n","            rd=random.sample(range(self.n_doc),self.n_doc)\n","            self.x=self.x[rd]\n","            self.y=self.y[rd]\n","            \n","        start=self.batch_id*self.batch_size\n","        end=start+self.batch_size\n","        return self.x[start:end],self.y[start:end]"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"7T5BAtSz1R2B","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592551330715,"user_tz":-420,"elapsed":1039,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["path_train='/content/drive/My Drive/Datasets/trainencode.txt'\n","path_test='/content/drive/My Drive/Datasets/testencode.txt'\n","path_vocab='/content/drive/My Drive/Datasets/vocab.txt'\n","\n","with open(path_vocab,'r') as f:\n","    vocab_size=len(f.read().splitlines())\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cliUWNDx1VEP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592551336146,"user_tz":-420,"elapsed":4017,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","class MyLSTM(nn.Module):\n","    def __init__(self,vocab_size,embed_size,hidden_size,num_class,dropout):\n","        super(MyLSTM,self).__init__()\n","        self._device=torch.device('cuda:0')\n","        self._num_class=num_class\n","        self._hidden_size=hidden_size\n","        self._embed_size=embed_size\n","        self._embed=nn.Embedding(vocab_size,embed_size)\n","        self._lstm=nn.LSTMCell(\n","            input_size=self._embed_size,\n","            hidden_size=self._hidden_size,\n","        )\n","        self._FC=nn.Linear(hidden_size,num_class)\n","        self._dropout=nn.Dropout(dropout)\n","    \n","    def forward(self,x):\n","        batch_size,sentence_len=x.size()\n","        x=self._embed(x)\n","        #x=self._dropout(x)\n","        x = x.permute(1,0,2)\n","        h=Variable(torch.zeros(batch_size,self._hidden_size).to(self._device))\n","        c=Variable(torch.zeros(batch_size,self._hidden_size).to(self._device))\n","        hs=[]\n","        for i in range(sentence_len):\n","            h,c=self._lstm(x[i],(h,c))\n","            hs.append(h)\n","        final_hidden_state, _ = torch.max(torch.stack(hs, 0), 0)\n","        output = self._FC(final_hidden_state)\n","        return output\n","\n","\n","class RNN_text_classification:\n","    def __init__(self,vocab_size,embed_size,batch_size,num_class,hidden_size,lr=0.001,dropout=0.5):\n","        self._vocab_size=vocab_size\n","        self._batch_size=batch_size\n","        self._num_class=num_class\n","        self._embed_size=embed_size\n","        self._hidden_size=hidden_size\n","        self._lr=lr\n","        self._dropout=dropout\n","        self._data_reader=None\n","        self._labels=None\n","        self.net=None\n","        self._device=torch.device('cuda:0')\n","    \n","    def train_and_pre(self,path_train,path_test,max_iter):\n","        # load data\n","        self._data_reader=DataReader(path_train,50)\n","        self._data_test=DataReader(path_test,50)\n","        # build net\n","        self._net=MyLSTM(\n","            vocab_size=self._vocab_size,\n","            embed_size=self._embed_size,\n","            hidden_size=self._hidden_size,\n","            num_class=self._num_class,\n","            dropout=self._dropout\n","        )\n","        self._net.to(self._device)  # Use GPU\n","        # loss function\n","        self._criterion=nn.CrossEntropyLoss()\n","        # use Optimizer Adam\n","        self._optimizer=torch.optim.Adam(self._net.parameters(),self._lr)\n","        self._net.train() # train state\n","        it=0\n","        while it<max_iter:\n","            # read batch and convert data to tensor\n","            train_data,train_labels=self._data_reader.next_batch()\n","            train_data=torch.from_numpy(train_data).to(self._device)\n","            train_labels=torch.from_numpy(train_labels).to(self._device)\n","            # clear gradient\n","            self._optimizer.zero_grad()\n","            # train\n","            labels_pre=self._net(train_data)\n","            #calculate loss\n","            loss=self._criterion(labels_pre,train_labels)\n","            # calculate gradient\n","            loss.backward()\n","            #update parameter\n","            self._optimizer.step()\n","\n","            if(self._data_reader.batch_id==0):\n","                print('Acc train = ',self.score(path_train).cpu().numpy(),\n","                      ' Acc test = ',self.score(path_test).cpu().numpy()\n","                )\n","\n","            if(self._data_reader.batch_id%20==0):\n","                print(\"epoch {}, step {} loss = {}\".format(\n","                    self._data_reader.epoch,self._data_reader.batch_id,loss.item()))\n","            it+=1\n","          \n","    def score(self,path_test):\n","        self._net.eval() # test state\n","        test_datareader=DataReader(path_test,50)\n","        num_pre_true=0\n","        while True:\n","            #load batch and convert data to tensor\n","            batch_test_data,batch_test_labels=test_datareader.next_batch()\n","            batch_test_data=torch.from_numpy(batch_test_data).to(self._device)\n","            batch_test_labels=torch.from_numpy(batch_test_labels).to(self._device)\n","            # test\n","            pre_vals = self._net(batch_test_data)\n","\n","            num_pre_true+=(torch.max(pre_vals.data, 1)[1].view(batch_test_labels.size()).data==batch_test_labels.data).sum()\n","            if(test_datareader.epoch==1):\n","                break\n","    \n","        return num_pre_true*100.0/len(test_datareader.y)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThgIdhhVEhOn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592529165799,"user_tz":-420,"elapsed":1074639,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"outputId":"3599ca57-75ce-4e9d-cffd-782a58648ac8"},"source":["import time\n","t=time.time()\n","model=RNN_text_classification(vocab_size,100,50,20,50,lr=0.01,dropout=0)\n","model.train_and_pre(path_train,path_test,5000)\n","print('time train= ',time.time()-t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch 0, step 20 loss = 2.9239461421966553\n","epoch 0, step 40 loss = 2.8760311603546143\n","epoch 0, step 60 loss = 2.8160345554351807\n","epoch 0, step 80 loss = 2.5497121810913086\n","epoch 0, step 100 loss = 2.3964099884033203\n","epoch 0, step 120 loss = 2.1210341453552246\n","epoch 0, step 140 loss = 1.941153883934021\n","epoch 0, step 160 loss = 1.573564052581787\n","epoch 0, step 180 loss = 1.1989617347717285\n","epoch 0, step 200 loss = 1.3283618688583374\n","epoch 0, step 220 loss = 1.2352972030639648\n","Acc train =  73.78469  Acc test =  59.174187\n","epoch 1, step 0 loss = 0.7683546543121338\n","epoch 1, step 20 loss = 0.7651944160461426\n","epoch 1, step 40 loss = 0.8229925632476807\n","epoch 1, step 60 loss = 0.8404043316841125\n","epoch 1, step 80 loss = 0.8041346073150635\n","epoch 1, step 100 loss = 0.7095690965652466\n","epoch 1, step 120 loss = 0.6430947780609131\n","epoch 1, step 140 loss = 1.0236449241638184\n","epoch 1, step 160 loss = 0.7456151843070984\n","epoch 1, step 180 loss = 0.848235547542572\n","epoch 1, step 200 loss = 0.7775197625160217\n","epoch 1, step 220 loss = 0.5844786167144775\n","Acc train =  88.75729  Acc test =  68.85289\n","epoch 2, step 0 loss = 0.4997958242893219\n","epoch 2, step 20 loss = 0.7188491821289062\n","epoch 2, step 40 loss = 0.5021706223487854\n","epoch 2, step 60 loss = 0.5171880125999451\n","epoch 2, step 80 loss = 0.39723101258277893\n","epoch 2, step 100 loss = 0.4167517125606537\n","epoch 2, step 120 loss = 0.18775252997875214\n","epoch 2, step 140 loss = 0.5154238939285278\n","epoch 2, step 160 loss = 0.38235461711883545\n","epoch 2, step 180 loss = 0.3837352693080902\n","epoch 2, step 200 loss = 0.5852418541908264\n","epoch 2, step 220 loss = 0.4822474718093872\n","Acc train =  94.131165  Acc test =  71.12321\n","epoch 3, step 0 loss = 0.22934459149837494\n","epoch 3, step 20 loss = 0.24944129586219788\n","epoch 3, step 40 loss = 0.246462881565094\n","epoch 3, step 60 loss = 0.21044687926769257\n","epoch 3, step 80 loss = 0.27783194184303284\n","epoch 3, step 100 loss = 0.3035520613193512\n","epoch 3, step 120 loss = 0.20954670011997223\n","epoch 3, step 140 loss = 0.14515909552574158\n","epoch 3, step 160 loss = 0.1495291292667389\n","epoch 3, step 180 loss = 0.10109653323888779\n","epoch 3, step 200 loss = 0.0899173766374588\n","epoch 3, step 220 loss = 0.3418157696723938\n","Acc train =  96.65017  Acc test =  72.59692\n","epoch 4, step 0 loss = 0.1297730952501297\n","epoch 4, step 20 loss = 0.17928020656108856\n","epoch 4, step 40 loss = 0.14210306107997894\n","epoch 4, step 60 loss = 0.1678430140018463\n","epoch 4, step 80 loss = 0.1867636740207672\n","epoch 4, step 100 loss = 0.11394507437944412\n","epoch 4, step 120 loss = 0.24238915741443634\n","epoch 4, step 140 loss = 0.13342627882957458\n","epoch 4, step 160 loss = 0.09719568490982056\n","epoch 4, step 180 loss = 0.09187863022089005\n","epoch 4, step 200 loss = 0.13364200294017792\n","epoch 4, step 220 loss = 0.27529284358024597\n","Acc train =  98.41789  Acc test =  73.68561\n","epoch 5, step 0 loss = 0.1989738494157791\n","epoch 5, step 20 loss = 0.0969095453619957\n","epoch 5, step 40 loss = 0.092619888484478\n","epoch 5, step 60 loss = 0.25974440574645996\n","epoch 5, step 80 loss = 0.053875621408224106\n","epoch 5, step 100 loss = 0.09563295543193817\n","epoch 5, step 120 loss = 0.016728658229112625\n","epoch 5, step 140 loss = 0.07551231980323792\n","epoch 5, step 160 loss = 0.1434718370437622\n","epoch 5, step 180 loss = 0.07068304717540741\n","epoch 5, step 200 loss = 0.17940634489059448\n","epoch 5, step 220 loss = 0.090695820748806\n","Acc train =  99.20453  Acc test =  73.19437\n","epoch 6, step 0 loss = 0.04430529475212097\n","epoch 6, step 20 loss = 0.029459552839398384\n","epoch 6, step 40 loss = 0.030617866665124893\n","epoch 6, step 60 loss = 0.019423436373472214\n","epoch 6, step 80 loss = 0.03183373436331749\n","epoch 6, step 100 loss = 0.032145414501428604\n","epoch 6, step 120 loss = 0.029935112223029137\n","epoch 6, step 140 loss = 0.03223908320069313\n","epoch 6, step 160 loss = 0.03725249320268631\n","epoch 6, step 180 loss = 0.09469934552907944\n","epoch 6, step 200 loss = 0.029515456408262253\n","epoch 6, step 220 loss = 0.020718440413475037\n","Acc train =  99.58459  Acc test =  73.32713\n","epoch 7, step 0 loss = 0.03014148771762848\n","epoch 7, step 20 loss = 0.014660511165857315\n","epoch 7, step 40 loss = 0.021406488493084908\n","epoch 7, step 60 loss = 0.015200681984424591\n","epoch 7, step 80 loss = 0.025063371285796165\n","epoch 7, step 100 loss = 0.03668534755706787\n","epoch 7, step 120 loss = 0.029103603214025497\n","epoch 7, step 140 loss = 0.04585208743810654\n","epoch 7, step 160 loss = 0.032269954681396484\n","epoch 7, step 180 loss = 0.023277683183550835\n","epoch 7, step 200 loss = 0.014826945960521698\n","epoch 7, step 220 loss = 0.03907023370265961\n","Acc train =  99.75252  Acc test =  73.93787\n","epoch 8, step 0 loss = 0.016966572031378746\n","epoch 8, step 20 loss = 0.011201181448996067\n","epoch 8, step 40 loss = 0.011601915583014488\n","epoch 8, step 60 loss = 0.011379671283066273\n","epoch 8, step 80 loss = 0.011971378698945045\n","epoch 8, step 100 loss = 0.04673086106777191\n","epoch 8, step 120 loss = 0.009079361334443092\n","epoch 8, step 140 loss = 0.010902595706284046\n","epoch 8, step 160 loss = 0.010597257874906063\n","epoch 8, step 180 loss = 0.021795015782117844\n","epoch 8, step 200 loss = 0.023260192945599556\n","epoch 8, step 220 loss = 0.019139671698212624\n","Acc train =  99.90278  Acc test =  74.044075\n","epoch 9, step 0 loss = 0.009221839718520641\n","epoch 9, step 20 loss = 0.008104200474917889\n","epoch 9, step 40 loss = 0.009269180707633495\n","epoch 9, step 60 loss = 0.005279073491692543\n","epoch 9, step 80 loss = 0.012129468843340874\n","epoch 9, step 100 loss = 0.014630689285695553\n","epoch 9, step 120 loss = 0.005743675399571657\n","epoch 9, step 140 loss = 0.0038260079454630613\n","epoch 9, step 160 loss = 0.010840130038559437\n","epoch 9, step 180 loss = 0.005226822104305029\n","epoch 9, step 200 loss = 0.02846149355173111\n","epoch 9, step 220 loss = 0.007322072982788086\n","Acc train =  99.92929  Acc test =  74.28306\n","epoch 10, step 0 loss = 0.005743427202105522\n","epoch 10, step 20 loss = 0.0038054180331528187\n","epoch 10, step 40 loss = 0.00326881418004632\n","epoch 10, step 60 loss = 0.007830295711755753\n","epoch 10, step 80 loss = 0.01830672286450863\n","epoch 10, step 100 loss = 0.002067089080810547\n","epoch 10, step 120 loss = 0.004112911410629749\n","epoch 10, step 140 loss = 0.0036543465685099363\n","epoch 10, step 160 loss = 0.0064943027682602406\n","epoch 10, step 180 loss = 0.007426423951983452\n","epoch 10, step 200 loss = 0.02586814947426319\n","epoch 10, step 220 loss = 0.0038793087005615234\n","Acc train =  99.94697  Acc test =  74.54859\n","epoch 11, step 0 loss = 0.00275528896600008\n","epoch 11, step 20 loss = 0.005837049335241318\n","epoch 11, step 40 loss = 0.0021273994352668524\n","epoch 11, step 60 loss = 0.0029300975147634745\n","epoch 11, step 80 loss = 0.001883325632661581\n","epoch 11, step 100 loss = 0.004256925545632839\n","epoch 11, step 120 loss = 0.0028925896622240543\n","epoch 11, step 140 loss = 0.004486513324081898\n","epoch 11, step 160 loss = 0.012381458654999733\n","epoch 11, step 180 loss = 0.027242880314588547\n","epoch 11, step 200 loss = 0.00809977576136589\n","epoch 11, step 220 loss = 0.00659523019567132\n","Acc train =  99.95581  Acc test =  74.68136\n","epoch 12, step 0 loss = 0.0027318859938532114\n","epoch 12, step 20 loss = 0.00452983845025301\n","epoch 12, step 40 loss = 0.009481735527515411\n","epoch 12, step 60 loss = 0.001805477193556726\n","epoch 12, step 80 loss = 0.0052160643972456455\n","epoch 12, step 100 loss = 0.005153779871761799\n","epoch 12, step 120 loss = 0.00371583946980536\n","epoch 12, step 140 loss = 0.0024765015114098787\n","epoch 12, step 160 loss = 0.007069807033985853\n","epoch 12, step 180 loss = 0.003546867286786437\n","epoch 12, step 200 loss = 0.003641328774392605\n","epoch 12, step 220 loss = 0.005463151726871729\n","Acc train =  99.98232  Acc test =  74.66808\n","epoch 13, step 0 loss = 0.0022614956833422184\n","epoch 13, step 20 loss = 0.00872798915952444\n","epoch 13, step 40 loss = 0.0027771187014877796\n","epoch 13, step 60 loss = 0.0032315636053681374\n","epoch 13, step 80 loss = 0.004127855412662029\n","epoch 13, step 100 loss = 0.004166145343333483\n","epoch 13, step 120 loss = 0.0026719856541603804\n","epoch 13, step 140 loss = 0.0013634300557896495\n","epoch 13, step 160 loss = 0.0013018036261200905\n","epoch 13, step 180 loss = 0.0011730194091796875\n","epoch 13, step 200 loss = 0.009475860744714737\n","epoch 13, step 220 loss = 0.005049305036664009\n","Acc train =  99.97349  Acc test =  74.56187\n","epoch 14, step 0 loss = 0.0010285377502441406\n","epoch 14, step 20 loss = 0.001331758452579379\n","epoch 14, step 40 loss = 0.015588388778269291\n","epoch 14, step 60 loss = 0.000845413189381361\n","epoch 14, step 80 loss = 0.0007384109776467085\n","epoch 14, step 100 loss = 0.0015927409986034036\n","epoch 14, step 120 loss = 0.0013321113074198365\n","epoch 14, step 140 loss = 0.0015476607950404286\n","epoch 14, step 160 loss = 0.0010498237097635865\n","epoch 14, step 180 loss = 0.0032342339400202036\n","epoch 14, step 200 loss = 0.0014197921846061945\n","epoch 14, step 220 loss = 0.0017271519172936678\n","Acc train =  99.97349  Acc test =  74.45565\n","epoch 15, step 0 loss = 0.0017291641561314464\n","epoch 15, step 20 loss = 0.003735198872163892\n","epoch 15, step 40 loss = 0.0021772575564682484\n","epoch 15, step 60 loss = 0.002320251427590847\n","epoch 15, step 80 loss = 0.0014080905821174383\n","epoch 15, step 100 loss = 0.006727810017764568\n","epoch 15, step 120 loss = 0.00394953740760684\n","epoch 15, step 140 loss = 0.0019147682469338179\n","epoch 15, step 160 loss = 0.004060029983520508\n","epoch 15, step 180 loss = 0.0017364787636324763\n","epoch 15, step 200 loss = 0.0074118804186582565\n","epoch 15, step 220 loss = 0.0016224670689553022\n","Acc train =  99.98232  Acc test =  74.33617\n","epoch 16, step 0 loss = 0.0008895873907022178\n","epoch 16, step 20 loss = 0.0015130615793168545\n","epoch 16, step 40 loss = 0.0017138480907306075\n","epoch 16, step 60 loss = 0.0027838326059281826\n","epoch 16, step 80 loss = 0.0015274047618731856\n","epoch 16, step 100 loss = 0.0014740371843799949\n","epoch 16, step 120 loss = 0.0008329391712322831\n","epoch 16, step 140 loss = 0.002668724162504077\n","epoch 16, step 160 loss = 0.0011237716535106301\n","epoch 16, step 180 loss = 0.0016834258567541838\n","epoch 16, step 200 loss = 0.0007945251418277621\n","epoch 16, step 220 loss = 0.0009013366652652621\n","Acc train =  99.99116  Acc test =  74.53532\n","epoch 17, step 0 loss = 0.002023773267865181\n","epoch 17, step 20 loss = 0.00421703327447176\n","epoch 17, step 40 loss = 0.0007008361862972379\n","epoch 17, step 60 loss = 0.0010692786891013384\n","epoch 17, step 80 loss = 0.001973276026546955\n","epoch 17, step 100 loss = 0.0011643219040706754\n","epoch 17, step 120 loss = 0.0007039642077870667\n","epoch 17, step 140 loss = 0.004602108150720596\n","epoch 17, step 160 loss = 0.0015707206912338734\n","epoch 17, step 180 loss = 0.00037792205694131553\n","epoch 17, step 200 loss = 0.0007026672246865928\n","epoch 17, step 220 loss = 0.0012261105002835393\n","Acc train =  99.98232  Acc test =  74.36272\n","epoch 18, step 0 loss = 0.0006836509564891458\n","epoch 18, step 20 loss = 0.0013908386463299394\n","epoch 18, step 40 loss = 0.0009325409191660583\n","epoch 18, step 60 loss = 0.0012743186671286821\n","epoch 18, step 80 loss = 0.0014469623565673828\n","epoch 18, step 100 loss = 0.0008119201520457864\n","epoch 18, step 120 loss = 0.006572914309799671\n","epoch 18, step 140 loss = 0.0024046229664236307\n","epoch 18, step 160 loss = 0.0011056518414989114\n","epoch 18, step 180 loss = 0.001726016984321177\n","epoch 18, step 200 loss = 0.0009120941394940019\n","epoch 18, step 220 loss = 0.0035647391341626644\n","Acc train =  99.99116  Acc test =  74.19012\n","epoch 19, step 0 loss = 0.0006858634878881276\n","epoch 19, step 20 loss = 0.0009439277928322554\n","epoch 19, step 40 loss = 0.0006885528564453125\n","epoch 19, step 60 loss = 0.0017063713166862726\n","epoch 19, step 80 loss = 0.0005179595900699496\n","epoch 19, step 100 loss = 0.0007302474696189165\n","epoch 19, step 120 loss = 0.0007721710135228932\n","epoch 19, step 140 loss = 0.0009780025575309992\n","epoch 19, step 160 loss = 0.00037548065301962197\n","epoch 19, step 180 loss = 0.0019766329787671566\n","epoch 19, step 200 loss = 0.0005143356393091381\n","epoch 19, step 220 loss = 0.001280393567867577\n","Acc train =  99.99116  Acc test =  74.30961\n","epoch 20, step 0 loss = 0.00034614562173373997\n","epoch 20, step 20 loss = 0.0004806900105904788\n","epoch 20, step 40 loss = 0.0005583000020124018\n","epoch 20, step 60 loss = 0.0004072761512361467\n","epoch 20, step 80 loss = 0.006319656502455473\n","epoch 20, step 100 loss = 0.0008819770882837474\n","epoch 20, step 120 loss = 0.0006290435558184981\n","epoch 20, step 140 loss = 0.0012442970182746649\n","epoch 20, step 160 loss = 0.0007952499436214566\n","epoch 20, step 180 loss = 0.029485072940587997\n","epoch 20, step 200 loss = 0.005137491039931774\n","epoch 20, step 220 loss = 0.0006871604709886014\n","Acc train =  99.99116  Acc test =  74.415825\n","epoch 21, step 0 loss = 0.0014640236040577292\n","epoch 21, step 20 loss = 0.001468610716983676\n","epoch 21, step 40 loss = 0.0005177879356779158\n","epoch 21, step 60 loss = 0.0013714885571971536\n","epoch 21, step 80 loss = 0.0015331458998844028\n","epoch 21, step 100 loss = 0.0008595466497354209\n","epoch 21, step 120 loss = 0.0029024791438132524\n","epoch 21, step 140 loss = 0.002930698450654745\n","epoch 21, step 160 loss = 0.0007616615039296448\n","epoch 21, step 180 loss = 0.0027515983674675226\n","epoch 21, step 200 loss = 0.0018463325686752796\n","epoch 21, step 220 loss = 0.0021530913654714823\n","Acc train =  99.98232  Acc test =  74.68136\n","epoch 22, step 0 loss = 0.0009133339044637978\n","time train=  1073.001088142395\n"],"name":"stdout"}]}]}
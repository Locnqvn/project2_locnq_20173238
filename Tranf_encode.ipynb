{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tranf_encode.ipynb","provenance":[],"mount_file_id":"1COgy1-AV2dWar5on9IFJFROAVkTxUS0u","authorship_tag":"ABX9TyN5cAohf/pVtkOzeQLznxQ9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"duBFKVA15dY_","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","import numpy as np \n","import os\n","import re\n","\n","class Tranf_encode:\n","    def __init__(self,max_doc_length,min_df):\n","        self._vocab=[]\n","        self._max_doc_length=max_doc_length\n","        self._min_df=min_df\n","    \n","    def isappear(self,word,wordslist):\n","        l=0\n","        r=len(wordslist)-1\n","        while(l<=r):\n","            m=int((r+l)/2)\n","            if(word==wordslist[m]):\n","                return m\n","            else:\n","                if(word<wordslist[m]):\n","                    r=m-1\n","                else:\n","                    l=m+1\n","        return -1\n","\n","    def fit(self,path_train):\n","        def get_data_and_vocab(path_train):\n","            data=[]\n","            vocab_count=defaultdict(int)\n","            list_folder=os.listdir(path_train)\n","            list_folder.sort()\n","            i_folder=0\n","            # clear data and find vocabulary\n","            for folder in list_folder:\n","                path_folder=path_train+'/'+folder\n","                list_doc=os.listdir(path_folder)\n","                list_doc.sort()\n","                for doc in list_doc:\n","                    path_file=path_folder+'/'+doc\n","                    with open(path_file,'r',errors='ignore') as f:\n","                        text=f.read()\n","                    words=re.split('\\W+',text)\n","                    for word in words:\n","                        vocab_count[word]+=1\n","                    content=' '.join(words)\n","                    ifo_doc=str(i_folder)+'<fff>'+str(doc)+'<fff>'+content\n","                    data.append(ifo_doc)\n","                i_folder+=1\n","            \n","            self._vocab=[word for word in vocab_count.keys()\n","                                if vocab_count[word]>self._min_df]\n","            self._vocab.sort()\n","            return data\n","        \n","        data_train=get_data_and_vocab(path_train)\n","        data_train_encode=self.get_encode(data_train)\n","        return data_train_encode\n","        \n","    def get_encode(self,data):\n","        vocab_id=dict([word, self._vocab.index(word)+2] for word in self._vocab) \n","        padding_ID=0\n","        unknown_ID=1\n","        data_encode=[]\n","        for doc in data:\n","            label,doc_id,text=doc.split('<fff>')\n","            words=text.split()[:self._max_doc_length]\n","            doc_encode=[]\n","            for word in words:\n","                if self.isappear(word,self._vocab)!=-1:\n","                    doc_encode.append(str(vocab_id[word]))\n","                else:\n","                    doc_encode.append(str(unknown_ID))\n","            if(len(words)<self._max_doc_length):\n","                for _ in range(self._max_doc_length-len(words)):\n","                    doc_encode.append(str(padding_ID))\n","            data_encode.append(label+'<fff>'+doc_id+'<fff>'+str(len(words))+'<fff>'+' '.join(doc_encode))\n","        \n","        return data_encode\n","        \n","    def fit_tranf(self,path_in):\n","        data=[]\n","        list_folder=os.listdir(path_in)\n","        list_folder.sort()\n","        i_folder=0\n","        for folder in list_folder:\n","            path_folder=path_in+'/'+folder\n","            list_doc=os.listdir(path_folder)\n","            list_doc.sort()\n","            for doc in list_doc:\n","                path_doc=path_folder+'/'+doc\n","                with open(path_doc,'r',errors='ignore') as f:\n","                    text=f.read()\n","                words=re.split('\\W+',text)\n","                content=' '.join(words)\n","                ifo_doc=str(i_folder)+'<fff>'+doc+'<fff>'+content\n","                data.append(ifo_doc)\n","            i_folder+=1\n","        \n","        data_encode=self.get_encode(data)\n","        return data_encode\n","    \n","    def get_vocab(self):\n","        return self._vocab\n","\n","model=Tranf_encode(500,10)\n","train=model.fit('/content/drive/My Drive/Datasets/20news-bydate-train')\n","test=model.fit_tranf('/content/drive/My Drive/Datasets/20news-bydate-test')\n","vocab=model.get_vocab()\n","\n","with open('/content/drive/My Drive/Datasets/trainencode1.txt','w') as f:\n","    f.write('\\n'.join(train))\n","with open('/content/drive/My Drive/Datasets/testencode1.txt','w') as f:\n","    f.write('\\n'.join(test))\n","with open('/content/drive/My Drive/Datasets/vocab1.txt','w') as f:\n","    f.write('\\n'.join(vocab))\n"],"execution_count":null,"outputs":[]}]}
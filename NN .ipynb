{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-1eGxQoYCGF366ybjmIEW50R2pyddzFg","authorship_tag":"ABX9TyMWyglcrebUUlSlvPgAUMjD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"riYQNkBvvn3V","colab_type":"code","colab":{}},"source":["import torch\n","import time\n","import random\n","import torch.nn as nn \n","import numpy as np \n","from torch.autograd import Variable\n","from torch.nn.init import kaiming_uniform_\n","from torch.nn.init import xavier_uniform_\n","\n","\n","\n","class MyNet(nn.Module):\n","    def __init__(self,layers):\n","        super(MyNet,self).__init__()\n","        self.relu=nn.ReLU()\n","        self.hidden=nn.ModuleList()\n","        for input_size,output_size in zip(layers,layers[1:]):\n","            self.hidden.append(nn.Linear(input_size,output_size,bias=True))\n","        for i in range(len(layers)-1):\n","            kaiming_uniform_(self.hidden[i].weight,nonlinearity='relu')\n","        xavier_uniform_(self.hidden[-1].weight)\n","        \n","\n","    def forward(self,active):\n","        for i in range(len(self.hidden)-1):\n","            active=self.hidden[i](active)\n","            active=self.relu(active)\n","        active=self.hidden[-1](active)\n","        active=nn.Softmax(dim=1)(active)\n","        \n","        return active\n","    \n","\n","class NNs:\n","    def __init__(self,dim_feature,n_class,batch_size,n_epochs,layers_size,tol,lr=0.01):\n","        self.dim_feature=dim_feature\n","        self.n_class=n_class\n","        self.batch_size=batch_size\n","        self.n_epochs=n_epochs\n","        self.lr=lr\n","        self.layers_size=layers_size\n","        self.X_train=[]\n","        self.Y_train=[]\n","        self.tol=tol\n","    \n","    def get_batch(self,i,batch_size):\n","        batch_x=self.X_train[i*batch_size:i*batch_size+batch_size]\n","        batch_y=self.Y_train[i*batch_size:i*batch_size+batch_size]\n","        return batch_x,batch_y\n","    \n","    def fit(self,X_train,Y_train):\n","        rd=random.sample(range(len(Y_train)),len(Y_train))\n","        self.X_train=np.array(X_train)[rd]\n","        self.Y_train=np.array(Y_train)[rd]\n","        self.net=MyNet(self.layers_size)\n","        self.criterion=nn.CrossEntropyLoss()\n","        self.optimizer=torch.optim.SGD(self.net.parameters(),self.lr,momentum=0.8)\n","        total_batch= int(np.ceil(self.X_train.shape[0]/self.batch_size))\n","\n","        self.net.train()\n","        for epoch in range(self.n_epochs):          \n","            for i in range(total_batch):\n","                # forward pass\n","                batch_x,batch_y=self.get_batch(i,self.batch_size)\n","                articles=Variable(torch.FloatTensor(batch_x))\n","                labels=Variable(torch.LongTensor(batch_y))\n","                self.optimizer.zero_grad()\n","                y_pre=self.net(articles)\n","\n","                #compute loss\n","                loss=self.criterion(y_pre,labels)\n","\n","                #backward pass\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                if(i%20==0):\n","                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n"," \t\t\t\t              %(epoch+1, self.n_epochs, i+1, total_batch, loss.item()))\n","                if(loss.item()<self.tol):\n","                    return\n","    \n","    def predict(self,X_test,Y_test):\n","        Y_test=np.array(Y_test)\n","        articles = Variable(torch.FloatTensor(X_test))\n","        outputs = self.net(articles)\n","        pre = torch.max(outputs.data, 1).indices.numpy()\n","        precision=sum(pre==Y_test)/len(Y_test)\n","    \n","        return precision\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fa-TT4V1ISyt","colab_type":"code","colab":{}},"source":["def loaddata(pathin):\n","    def convert_dense_vector(text,dim):\n","        v=np.zeros(dim)\n","        x_fea=text.split()\n","        for x in x_fea:\n","            fea=x.split(':')\n","            v[int(fea[0])]=float(fea[1])\n","        return v\n","\n","    with open(pathin,'r') as f:\n","        d_lines=f.read().splitlines()\n","    '''\n","    with open(\"/content/drive/My Drive/Project2/Datasets/words_idf\",'r') as f:\n","        dim=f.read().splitlines()\n","    '''\n","    dim=20167\n","\n","    X=[]\n","    Y=[]\n","    for doc in d_lines:\n","        fea_doc=doc.split('<<>>')\n","        Y.append(int(fea_doc[0]))\n","        X.append(convert_dense_vector(fea_doc[2],dim))\n","    \n","    return X,Y\n","\n","\n","x_train,y_train=loaddata(\"/content/drive/My Drive/Project2/Datasets/train_tfidf_df=3\")\n","x_test,y_test=loaddata(\"/content/drive/My Drive/Project2/Datasets/test_tfidf\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uP-nk5J_U-C0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0356d9d1-2e46-4d06-f35e-646482d03f8b","executionInfo":{"status":"ok","timestamp":1589948872178,"user_tz":-420,"elapsed":86542,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["# optimize with SGD , lr=0.01\n","layers_size=[20167,100,50,20]\n","t=time.time()\n","model=NNs(20167,20,100,30,layers_size=layers_size,lr=0.01,tol=2.08)\n","model.fit(x_train,y_train)\n","print(\"time train =\", time.time()-t)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Epoch [1/30], Step [1/114], Loss: 2.9956\n","Epoch [1/30], Step [21/114], Loss: 2.9959\n","Epoch [1/30], Step [41/114], Loss: 2.9964\n","Epoch [1/30], Step [61/114], Loss: 2.9952\n","Epoch [1/30], Step [81/114], Loss: 2.9962\n","Epoch [1/30], Step [101/114], Loss: 2.9954\n","Epoch [2/30], Step [1/114], Loss: 2.9954\n","Epoch [2/30], Step [21/114], Loss: 2.9957\n","Epoch [2/30], Step [41/114], Loss: 2.9962\n","Epoch [2/30], Step [61/114], Loss: 2.9951\n","Epoch [2/30], Step [81/114], Loss: 2.9960\n","Epoch [2/30], Step [101/114], Loss: 2.9953\n","Epoch [3/30], Step [1/114], Loss: 2.9953\n","Epoch [3/30], Step [21/114], Loss: 2.9956\n","Epoch [3/30], Step [41/114], Loss: 2.9960\n","Epoch [3/30], Step [61/114], Loss: 2.9949\n","Epoch [3/30], Step [81/114], Loss: 2.9959\n","Epoch [3/30], Step [101/114], Loss: 2.9951\n","Epoch [4/30], Step [1/114], Loss: 2.9951\n","Epoch [4/30], Step [21/114], Loss: 2.9954\n","Epoch [4/30], Step [41/114], Loss: 2.9958\n","Epoch [4/30], Step [61/114], Loss: 2.9947\n","Epoch [4/30], Step [81/114], Loss: 2.9957\n","Epoch [4/30], Step [101/114], Loss: 2.9949\n","Epoch [5/30], Step [1/114], Loss: 2.9949\n","Epoch [5/30], Step [21/114], Loss: 2.9952\n","Epoch [5/30], Step [41/114], Loss: 2.9956\n","Epoch [5/30], Step [61/114], Loss: 2.9945\n","Epoch [5/30], Step [81/114], Loss: 2.9955\n","Epoch [5/30], Step [101/114], Loss: 2.9946\n","Epoch [6/30], Step [1/114], Loss: 2.9947\n","Epoch [6/30], Step [21/114], Loss: 2.9950\n","Epoch [6/30], Step [41/114], Loss: 2.9954\n","Epoch [6/30], Step [61/114], Loss: 2.9942\n","Epoch [6/30], Step [81/114], Loss: 2.9953\n","Epoch [6/30], Step [101/114], Loss: 2.9944\n","Epoch [7/30], Step [1/114], Loss: 2.9945\n","Epoch [7/30], Step [21/114], Loss: 2.9948\n","Epoch [7/30], Step [41/114], Loss: 2.9951\n","Epoch [7/30], Step [61/114], Loss: 2.9940\n","Epoch [7/30], Step [81/114], Loss: 2.9950\n","Epoch [7/30], Step [101/114], Loss: 2.9942\n","Epoch [8/30], Step [1/114], Loss: 2.9942\n","Epoch [8/30], Step [21/114], Loss: 2.9946\n","Epoch [8/30], Step [41/114], Loss: 2.9949\n","Epoch [8/30], Step [61/114], Loss: 2.9937\n","Epoch [8/30], Step [81/114], Loss: 2.9948\n","Epoch [8/30], Step [101/114], Loss: 2.9939\n","Epoch [9/30], Step [1/114], Loss: 2.9940\n","Epoch [9/30], Step [21/114], Loss: 2.9943\n","Epoch [9/30], Step [41/114], Loss: 2.9946\n","Epoch [9/30], Step [61/114], Loss: 2.9934\n","Epoch [9/30], Step [81/114], Loss: 2.9945\n","Epoch [9/30], Step [101/114], Loss: 2.9936\n","Epoch [10/30], Step [1/114], Loss: 2.9938\n","Epoch [10/30], Step [21/114], Loss: 2.9940\n","Epoch [10/30], Step [41/114], Loss: 2.9943\n","Epoch [10/30], Step [61/114], Loss: 2.9931\n","Epoch [10/30], Step [81/114], Loss: 2.9942\n","Epoch [10/30], Step [101/114], Loss: 2.9933\n","Epoch [11/30], Step [1/114], Loss: 2.9935\n","Epoch [11/30], Step [21/114], Loss: 2.9938\n","Epoch [11/30], Step [41/114], Loss: 2.9940\n","Epoch [11/30], Step [61/114], Loss: 2.9928\n","Epoch [11/30], Step [81/114], Loss: 2.9939\n","Epoch [11/30], Step [101/114], Loss: 2.9930\n","Epoch [12/30], Step [1/114], Loss: 2.9932\n","Epoch [12/30], Step [21/114], Loss: 2.9935\n","Epoch [12/30], Step [41/114], Loss: 2.9936\n","Epoch [12/30], Step [61/114], Loss: 2.9924\n","Epoch [12/30], Step [81/114], Loss: 2.9935\n","Epoch [12/30], Step [101/114], Loss: 2.9926\n","Epoch [13/30], Step [1/114], Loss: 2.9928\n","Epoch [13/30], Step [21/114], Loss: 2.9931\n","Epoch [13/30], Step [41/114], Loss: 2.9933\n","Epoch [13/30], Step [61/114], Loss: 2.9920\n","Epoch [13/30], Step [81/114], Loss: 2.9931\n","Epoch [13/30], Step [101/114], Loss: 2.9922\n","Epoch [14/30], Step [1/114], Loss: 2.9925\n","Epoch [14/30], Step [21/114], Loss: 2.9928\n","Epoch [14/30], Step [41/114], Loss: 2.9929\n","Epoch [14/30], Step [61/114], Loss: 2.9915\n","Epoch [14/30], Step [81/114], Loss: 2.9927\n","Epoch [14/30], Step [101/114], Loss: 2.9918\n","Epoch [15/30], Step [1/114], Loss: 2.9921\n","Epoch [15/30], Step [21/114], Loss: 2.9924\n","Epoch [15/30], Step [41/114], Loss: 2.9924\n","Epoch [15/30], Step [61/114], Loss: 2.9910\n","Epoch [15/30], Step [81/114], Loss: 2.9922\n","Epoch [15/30], Step [101/114], Loss: 2.9913\n","Epoch [16/30], Step [1/114], Loss: 2.9917\n","Epoch [16/30], Step [21/114], Loss: 2.9919\n","Epoch [16/30], Step [41/114], Loss: 2.9919\n","Epoch [16/30], Step [61/114], Loss: 2.9904\n","Epoch [16/30], Step [81/114], Loss: 2.9917\n","Epoch [16/30], Step [101/114], Loss: 2.9908\n","Epoch [17/30], Step [1/114], Loss: 2.9912\n","Epoch [17/30], Step [21/114], Loss: 2.9914\n","Epoch [17/30], Step [41/114], Loss: 2.9914\n","Epoch [17/30], Step [61/114], Loss: 2.9897\n","Epoch [17/30], Step [81/114], Loss: 2.9911\n","Epoch [17/30], Step [101/114], Loss: 2.9902\n","Epoch [18/30], Step [1/114], Loss: 2.9907\n","Epoch [18/30], Step [21/114], Loss: 2.9909\n","Epoch [18/30], Step [41/114], Loss: 2.9908\n","Epoch [18/30], Step [61/114], Loss: 2.9890\n","Epoch [18/30], Step [81/114], Loss: 2.9905\n","Epoch [18/30], Step [101/114], Loss: 2.9895\n","Epoch [19/30], Step [1/114], Loss: 2.9901\n","Epoch [19/30], Step [21/114], Loss: 2.9903\n","Epoch [19/30], Step [41/114], Loss: 2.9901\n","Epoch [19/30], Step [61/114], Loss: 2.9882\n","Epoch [19/30], Step [81/114], Loss: 2.9898\n","Epoch [19/30], Step [101/114], Loss: 2.9888\n","Epoch [20/30], Step [1/114], Loss: 2.9895\n","Epoch [20/30], Step [21/114], Loss: 2.9897\n","Epoch [20/30], Step [41/114], Loss: 2.9893\n","Epoch [20/30], Step [61/114], Loss: 2.9873\n","Epoch [20/30], Step [81/114], Loss: 2.9890\n","Epoch [20/30], Step [101/114], Loss: 2.9880\n","Epoch [21/30], Step [1/114], Loss: 2.9888\n","Epoch [21/30], Step [21/114], Loss: 2.9890\n","Epoch [21/30], Step [41/114], Loss: 2.9884\n","Epoch [21/30], Step [61/114], Loss: 2.9862\n","Epoch [21/30], Step [81/114], Loss: 2.9882\n","Epoch [21/30], Step [101/114], Loss: 2.9871\n","Epoch [22/30], Step [1/114], Loss: 2.9880\n","Epoch [22/30], Step [21/114], Loss: 2.9882\n","Epoch [22/30], Step [41/114], Loss: 2.9875\n","Epoch [22/30], Step [61/114], Loss: 2.9851\n","Epoch [22/30], Step [81/114], Loss: 2.9872\n","Epoch [22/30], Step [101/114], Loss: 2.9861\n","Epoch [23/30], Step [1/114], Loss: 2.9871\n","Epoch [23/30], Step [21/114], Loss: 2.9872\n","Epoch [23/30], Step [41/114], Loss: 2.9864\n","Epoch [23/30], Step [61/114], Loss: 2.9837\n","Epoch [23/30], Step [81/114], Loss: 2.9861\n","Epoch [23/30], Step [101/114], Loss: 2.9849\n","Epoch [24/30], Step [1/114], Loss: 2.9861\n","Epoch [24/30], Step [21/114], Loss: 2.9862\n","Epoch [24/30], Step [41/114], Loss: 2.9851\n","Epoch [24/30], Step [61/114], Loss: 2.9821\n","Epoch [24/30], Step [81/114], Loss: 2.9848\n","Epoch [24/30], Step [101/114], Loss: 2.9836\n","Epoch [25/30], Step [1/114], Loss: 2.9849\n","Epoch [25/30], Step [21/114], Loss: 2.9850\n","Epoch [25/30], Step [41/114], Loss: 2.9836\n","Epoch [25/30], Step [61/114], Loss: 2.9803\n","Epoch [25/30], Step [81/114], Loss: 2.9833\n","Epoch [25/30], Step [101/114], Loss: 2.9820\n","Epoch [26/30], Step [1/114], Loss: 2.9835\n","Epoch [26/30], Step [21/114], Loss: 2.9837\n","Epoch [26/30], Step [41/114], Loss: 2.9819\n","Epoch [26/30], Step [61/114], Loss: 2.9780\n","Epoch [26/30], Step [81/114], Loss: 2.9817\n","Epoch [26/30], Step [101/114], Loss: 2.9802\n","Epoch [27/30], Step [1/114], Loss: 2.9818\n","Epoch [27/30], Step [21/114], Loss: 2.9822\n","Epoch [27/30], Step [41/114], Loss: 2.9799\n","Epoch [27/30], Step [61/114], Loss: 2.9753\n","Epoch [27/30], Step [81/114], Loss: 2.9797\n","Epoch [27/30], Step [101/114], Loss: 2.9780\n","Epoch [28/30], Step [1/114], Loss: 2.9797\n","Epoch [28/30], Step [21/114], Loss: 2.9804\n","Epoch [28/30], Step [41/114], Loss: 2.9775\n","Epoch [28/30], Step [61/114], Loss: 2.9718\n","Epoch [28/30], Step [81/114], Loss: 2.9772\n","Epoch [28/30], Step [101/114], Loss: 2.9753\n","Epoch [29/30], Step [1/114], Loss: 2.9770\n","Epoch [29/30], Step [21/114], Loss: 2.9783\n","Epoch [29/30], Step [41/114], Loss: 2.9745\n","Epoch [29/30], Step [61/114], Loss: 2.9672\n","Epoch [29/30], Step [81/114], Loss: 2.9741\n","Epoch [29/30], Step [101/114], Loss: 2.9718\n","Epoch [30/30], Step [1/114], Loss: 2.9735\n","Epoch [30/30], Step [21/114], Loss: 2.9757\n","Epoch [30/30], Step [41/114], Loss: 2.9705\n","Epoch [30/30], Step [61/114], Loss: 2.9609\n","Epoch [30/30], Step [81/114], Loss: 2.9701\n","Epoch [30/30], Step [101/114], Loss: 2.9672\n","time train = 85.89714908599854\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ogy5OBJ9WAzI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"395babae-642e-4e9b-d8d8-1f8ce1ec78b5","executionInfo":{"status":"ok","timestamp":1589949021873,"user_tz":-420,"elapsed":40507,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["# optimize with SGD , lr=0.01\n","print(\"test=\",model.predict(x_test,y_test))\n","print(\"train=\",model.predict(x_train,y_train))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["test= 0.1327668613913967\n","train= 0.15953685699133816\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E0dLf03RFRHr","colab_type":"code","outputId":"1834b890-5ba6-42d6-ec60-26d297882eac","executionInfo":{"status":"ok","timestamp":1589948644216,"user_tz":-420,"elapsed":36630,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# optimize with SGD , lr=0.5\n","layers_size=[20167,100,50,20]\n","t=time.time()\n","model=NNs(20167,20,100,30,layers_size=layers_size,lr=0.5,tol=2.08)\n","model.fit(x_train,y_train)\n","print(\"time train =\", time.time()-t)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch [1/30], Step [1/114], Loss: 2.9963\n","Epoch [1/30], Step [21/114], Loss: 2.9947\n","Epoch [1/30], Step [41/114], Loss: 2.9929\n","Epoch [1/30], Step [61/114], Loss: 2.9872\n","Epoch [1/30], Step [81/114], Loss: 2.9609\n","Epoch [1/30], Step [101/114], Loss: 2.8637\n","Epoch [2/30], Step [1/114], Loss: 2.7711\n","Epoch [2/30], Step [21/114], Loss: 2.7045\n","Epoch [2/30], Step [41/114], Loss: 2.5253\n","Epoch [2/30], Step [61/114], Loss: 2.4980\n","Epoch [2/30], Step [81/114], Loss: 2.4497\n","Epoch [2/30], Step [101/114], Loss: 2.3561\n","Epoch [3/30], Step [1/114], Loss: 2.3955\n","Epoch [3/30], Step [21/114], Loss: 2.3656\n","Epoch [3/30], Step [41/114], Loss: 2.3636\n","Epoch [3/30], Step [61/114], Loss: 2.3757\n","Epoch [3/30], Step [81/114], Loss: 2.3504\n","Epoch [3/30], Step [101/114], Loss: 2.2647\n","Epoch [4/30], Step [1/114], Loss: 2.3010\n","Epoch [4/30], Step [21/114], Loss: 2.2713\n","Epoch [4/30], Step [41/114], Loss: 2.3076\n","Epoch [4/30], Step [61/114], Loss: 2.2895\n","Epoch [4/30], Step [81/114], Loss: 2.2540\n","Epoch [4/30], Step [101/114], Loss: 2.2220\n","Epoch [5/30], Step [1/114], Loss: 2.2966\n","Epoch [5/30], Step [21/114], Loss: 2.2411\n","Epoch [5/30], Step [41/114], Loss: 2.2994\n","Epoch [5/30], Step [61/114], Loss: 2.2803\n","Epoch [5/30], Step [81/114], Loss: 2.2435\n","Epoch [5/30], Step [101/114], Loss: 2.2165\n","Epoch [6/30], Step [1/114], Loss: 2.2956\n","Epoch [6/30], Step [21/114], Loss: 2.2360\n","Epoch [6/30], Step [41/114], Loss: 2.2848\n","Epoch [6/30], Step [61/114], Loss: 2.2762\n","Epoch [6/30], Step [81/114], Loss: 2.2372\n","Epoch [6/30], Step [101/114], Loss: 2.2071\n","Epoch [7/30], Step [1/114], Loss: 2.2948\n","Epoch [7/30], Step [21/114], Loss: 2.2359\n","Epoch [7/30], Step [41/114], Loss: 2.2842\n","Epoch [7/30], Step [61/114], Loss: 2.2757\n","Epoch [7/30], Step [81/114], Loss: 2.2367\n","Epoch [7/30], Step [101/114], Loss: 2.2161\n","Epoch [8/30], Step [1/114], Loss: 2.2948\n","Epoch [8/30], Step [21/114], Loss: 2.2355\n","Epoch [8/30], Step [41/114], Loss: 2.2837\n","Epoch [8/30], Step [61/114], Loss: 2.2756\n","Epoch [8/30], Step [81/114], Loss: 2.2361\n","Epoch [8/30], Step [101/114], Loss: 2.2052\n","Epoch [9/30], Step [1/114], Loss: 2.2808\n","Epoch [9/30], Step [21/114], Loss: 2.2066\n","Epoch [9/30], Step [41/114], Loss: 2.1865\n","Epoch [9/30], Step [61/114], Loss: 2.2340\n","Epoch [9/30], Step [81/114], Loss: 2.1746\n","Epoch [9/30], Step [101/114], Loss: 2.1624\n","Epoch [10/30], Step [1/114], Loss: 2.2077\n","Epoch [10/30], Step [21/114], Loss: 2.1484\n","Epoch [10/30], Step [41/114], Loss: 2.1580\n","Epoch [10/30], Step [61/114], Loss: 2.2282\n","Epoch [10/30], Step [81/114], Loss: 2.1467\n","Epoch [10/30], Step [101/114], Loss: 2.1496\n","Epoch [11/30], Step [1/114], Loss: 2.1869\n","Epoch [11/30], Step [21/114], Loss: 2.1468\n","Epoch [11/30], Step [41/114], Loss: 2.1428\n","Epoch [11/30], Step [61/114], Loss: 2.2070\n","Epoch [11/30], Step [81/114], Loss: 2.1278\n","Epoch [11/30], Step [101/114], Loss: 2.1161\n","Epoch [12/30], Step [1/114], Loss: 2.1053\n","Epoch [12/30], Step [21/114], Loss: 2.1174\n","Epoch [12/30], Step [41/114], Loss: 2.1103\n","Epoch [12/30], Step [61/114], Loss: 2.1418\n","Epoch [12/30], Step [81/114], Loss: 2.0985\n","Epoch [12/30], Step [101/114], Loss: 2.1093\n","time train = 35.28295350074768\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NiBiAxCrYTI8","colab_type":"code","outputId":"ad7877ac-b01e-48ab-9a36-0207d3356c27","executionInfo":{"status":"ok","timestamp":1589948723741,"user_tz":-420,"elapsed":40122,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# optimize with SGD , lr=0.5\n","print(\"test=\",model.predict(x_test,y_test))\n","print(\"train=\",model.predict(x_train,y_train))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["test= 0.7701805629314923\n","train= 0.9546579459077249\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AWMFMcPNnjP0","colab_type":"code","outputId":"59811f20-bb29-4b9d-8a8d-5876dee4680a","executionInfo":{"status":"ok","timestamp":1589948049946,"user_tz":-420,"elapsed":13399,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# optimize with Adam, lr=0.001\n","layers_size=[20167,100,50,20]\n","t=time.time()\n","model=NNs(20167,20,100,30,layers_size=layers_size,lr=0.001,tol=2.08)\n","model.fit(x_train,y_train)\n","print(\"time train =\", time.time()-t)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch [1/30], Step [1/114], Loss: 2.9965\n","Epoch [1/30], Step [21/114], Loss: 2.9660\n","Epoch [1/30], Step [41/114], Loss: 2.7705\n","Epoch [1/30], Step [61/114], Loss: 2.5819\n","Epoch [1/30], Step [81/114], Loss: 2.4951\n","Epoch [1/30], Step [101/114], Loss: 2.3079\n","Epoch [2/30], Step [1/114], Loss: 2.2713\n","Epoch [2/30], Step [21/114], Loss: 2.3249\n","Epoch [2/30], Step [41/114], Loss: 2.1862\n","Epoch [2/30], Step [61/114], Loss: 2.1564\n","Epoch [2/30], Step [81/114], Loss: 2.1728\n","Epoch [2/30], Step [101/114], Loss: 2.1365\n","Epoch [3/30], Step [1/114], Loss: 2.1302\n","Epoch [3/30], Step [21/114], Loss: 2.1332\n","Epoch [3/30], Step [41/114], Loss: 2.1116\n","Epoch [3/30], Step [61/114], Loss: 2.1141\n","Epoch [3/30], Step [81/114], Loss: 2.0975\n","Epoch [3/30], Step [101/114], Loss: 2.0932\n","time train = 12.344376564025879\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zu4Yv8mUsX9t","colab_type":"code","outputId":"b6eb36e3-19f8-4a97-82f1-be35a758b256","executionInfo":{"status":"ok","timestamp":1589948093917,"user_tz":-420,"elapsed":39420,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# optimize with Adam , lr=0.001\n","print(\"test=\",model.predict(x_test,y_test))\n","print(\"train=\",model.predict(x_train,y_train))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["test= 0.8368295273499734\n","train= 0.9880678805020329\n"],"name":"stdout"}]}]}